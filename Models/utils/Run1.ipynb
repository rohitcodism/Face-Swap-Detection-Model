{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d427c0897c34fdf8",
   "metadata": {},
   "source": [
    "# Introduction to the Integrated Deepfake Detection System\n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to the Integrated Deepfake Detection System! This project is a comprehensive effort to tackle the growing problem of deepfake videos using advanced machine learning techniques. As deepfake technology continues to evolve, it poses significant challenges to privacy, security, and authenticity in media. Our system is designed to detect deepfakes by analyzing multiple aspects of video content, specifically focusing on spatial, temporal, and micro-expression features. By integrating these different feature types, we aim to create a robust and reliable detection mechanism capable of identifying manipulated video content.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The primary objective of this project is to develop a deepfake detection system that can accurately distinguish between genuine and manipulated videos. This involves:\n",
    "\n",
    "1. **Spatial Feature Extraction**: Analyzing individual frames of a video to capture static facial features. This is accomplished using pre-trained Convolutional Neural Networks (CNNs) such as ResNet50 and VGG16, which are fine-tuned for this task.\n",
    "   \n",
    "2. **Temporal Feature Extraction**: Understanding how facial features change over time by analyzing sequences of frames. Bidirectional Long Short-Term Memory (BiLSTM) networks are employed to capture temporal dependencies and detect inconsistencies in the flow of facial expressions.\n",
    "\n",
    "3. **Micro-Expression Analysis**: Focusing on subtle facial movements that are difficult to replicate in deepfake videos. This module uses specialized CNN architectures to extract and analyze micro-expressions, providing an additional layer of detection.\n",
    "\n",
    "4. **Feature Fusion**: Combining spatial, temporal, and micro-expression features using attention mechanisms to form a comprehensive feature set that enhances detection accuracy. This fusion approach leverages the strengths of each feature type to make a final decision about the authenticity of the video.\n",
    "\n",
    "## Why This Approach?\n",
    "\n",
    "Deepfake detection is a challenging task due to the sophistication of the algorithms used to create these fakes. Traditional detection methods that rely on a single type of feature often fail to capture the complexity of manipulations. By integrating spatial, temporal, and micro-expression features, our approach provides a multi-dimensional analysis that significantly improves the likelihood of detecting deepfakes. This holistic strategy addresses the limitations of existing methods and provides a more reliable solution.\n",
    "\n",
    "### Key Challenges Addressed:\n",
    "\n",
    "- **Variability in Deepfake Techniques**: Different deepfake algorithms have varying strengths and weaknesses. By analyzing multiple aspects of the video, our system can detect a wide range of manipulations.\n",
    "- **Subtle Manipulations**: Some deepfakes are so well-crafted that the manipulations are not immediately noticeable to the human eye. Micro-expression analysis helps in detecting these subtle manipulations.\n",
    "- **Generalization**: Ensuring that the model is not overfitting to specific datasets or types of deepfakes. The fusion of different feature types helps the model generalize better across diverse datasets.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "For this project, we use the **FaceForensics++** dataset, which is a benchmark dataset commonly used in deepfake detection research. It contains a collection of both original and manipulated video sequences, providing a diverse set of examples for training and testing the system. The dataset is divided into two main categories:\n",
    "\n",
    "- **Original Sequences**: Videos that have not been altered, serving as ground truth for authenticity.\n",
    "- **Manipulated Sequences**: Videos that have been altered using various deepfake techniques, providing examples of fake content.\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "This Jupyter Notebook is structured to guide you through the different stages of the deepfake detection process:\n",
    "\n",
    "1. **Data Preprocessing**: Preparing the video frames for feature extraction, including face detection, alignment, and normalization.\n",
    "   \n",
    "2. **Model Architecture**: Detailed implementation of the spatial, temporal, and micro-expression feature extraction models. This section includes building and training the CNN (ResNet50, VGG16), BiLSTM, and micro-expression analysis models.\n",
    "\n",
    "3. **Feature Fusion and Classification**: Combining the extracted features and using attention mechanisms to improve the detection accuracy. The final output layer provides the classification result, indicating whether the video is genuine or a deepfake.\n",
    "\n",
    "4. **Evaluation and Results**: Testing the trained model on a set of test videos and evaluating its performance using metrics such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "5. **Conclusion and Future Work**: Summarizing the findings and discussing potential improvements and future directions for research in deepfake detection.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "To run this notebook, ensure that you have all the necessary dependencies installed. The required libraries are listed in the `requirements.txt` file. Use the following command to install them:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Additionally, make sure you have the FaceForensics++ dataset downloaded and placed in the appropriate directory as specified in the notebook.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The Integrated Deepfake Detection System represents a significant step towards addressing the challenges posed by deepfake technology. By leveraging multiple feature extraction methods and incorporating attention mechanisms, this system aims to provide a robust solution for identifying manipulated video content. We hope that this project will contribute to the ongoing efforts to maintain the integrity and authenticity of digital media.\n",
    "\n",
    "Let's get started and dive into the world of deepfake detection!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9540ebef66cb2a0",
   "metadata": {},
   "source": [
    "## **Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c3172c61182ab4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T07:48:14.978950Z",
     "start_time": "2024-09-15T07:48:14.189781Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b4580d4832d031",
   "metadata": {},
   "source": [
    "## **Preprocessing Phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5da46b4e2841c322",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T08:22:29.456458Z",
     "start_time": "2024-09-14T08:22:29.453185Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7953a0c42f06ed",
   "metadata": {},
   "source": [
    "## **Model Building**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5196a483ec9cd14",
   "metadata": {},
   "source": [
    "## 1. **Facial Feature Extractor Module**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba19f2fcb81df0d0",
   "metadata": {},
   "source": [
    "### 1. Spatial Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T08:22:36.069953Z",
     "start_time": "2024-09-14T08:22:29.489204Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.api.layers import TimeDistributed,Input, Flatten, GlobalAvgPool1D\n",
    "from keras.api.applications.resnet50 import  ResNet50, preprocess_input as resnet_preprocess\n",
    "from keras.api.models import  Model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe90f1b1c190f6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T08:22:37.308826Z",
     "start_time": "2024-09-14T08:22:36.069953Z"
    }
   },
   "outputs": [],
   "source": [
    "base_model = ResNet50(include_top=False, weights='imagenet', pooling='avg', input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd898bcb7d806b01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T08:22:37.318446Z",
     "start_time": "2024-09-14T08:22:37.308826Z"
    }
   },
   "outputs": [],
   "source": [
    "spatial_model =  Model(inputs=base_model.input, outputs=base_model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baa2dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in spatial_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e6fb47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 2048)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spatial_model.output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f30f05",
   "metadata": {},
   "source": [
    "### 2. Temporal Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2294abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Bidirectional,LSTM,Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "601c81d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_temporal_feature_extractor():\n",
    "    input_seq = Input(shape=(30,2048))\n",
    "\n",
    "    lstm_1 = LSTM(128, return_sequences=True, dropout=0.2)\n",
    "    lstm_2 = LSTM(64,return_sequences=True, dropout=0.2)\n",
    "\n",
    "    lstm_out = Bidirectional(lstm_1)(input_seq)\n",
    "    lstm_out = Bidirectional(lstm_2)(lstm_out)\n",
    "\n",
    "    model = Model(inputs=input_seq, outputs=lstm_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4522e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_model = build_temporal_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "246f329b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor shape=(None, 30, 128), dtype=float32, sparse=False, name=keras_tensor_178>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temporal_model.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8839ed419f1aec",
   "metadata": {},
   "source": [
    "## **2. Micro Expression Inconsistency Detection Module**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7483995b04b067",
   "metadata": {},
   "source": [
    "### **1. Micro Expression Feature Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e395e308720792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.api.layers import Conv2D,BatchNormalization,Activation,MaxPooling2D,Dropout, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bffd388828a69382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_micro_exp_spatial_feature_extractor():\n",
    "    spatial_inputs = Input(shape=(64,64,3))\n",
    "\n",
    "    # Layer 1\n",
    "    micro_exp_x = Conv2D(32,(3,3),padding='same')(spatial_inputs)\n",
    "    micro_exp_x = BatchNormalization()(micro_exp_x)\n",
    "    micro_exp_x = Activation('relu')(micro_exp_x)\n",
    "    micro_exp_x = MaxPooling2D(pool_size=(2,2))(micro_exp_x)\n",
    "    \n",
    "    # Layer 2\n",
    "    micro_exp_x = Conv2D(64,(3,3),padding='same')(micro_exp_x)\n",
    "    micro_exp_x = BatchNormalization()(micro_exp_x)\n",
    "    micro_exp_x = Activation('relu')(micro_exp_x)\n",
    "    micro_exp_x = MaxPooling2D(pool_size=(2,2))(micro_exp_x)\n",
    "    \n",
    "    # Layer 3\n",
    "    micro_exp_x = Conv2D(128,(3,3),padding='same')(micro_exp_x)\n",
    "    micro_exp_x = BatchNormalization()(micro_exp_x)\n",
    "    micro_exp_x = Activation('relu')(micro_exp_x)\n",
    "    micro_exp_x = MaxPooling2D(pool_size=(2,2))(micro_exp_x)\n",
    "    \n",
    "    # Layer 4\n",
    "    micro_exp_x = Flatten()(micro_exp_x)\n",
    "    micro_exp_x = Dense(256, activation='relu')(micro_exp_x)\n",
    "    # Add dropouts in case of overfitting\n",
    "    \n",
    "    micro_exp_output = Dense(128, activation='relu')(micro_exp_x)\n",
    "    \n",
    "    micro_exp_spatial_feature_extractor = Model(inputs=spatial_inputs, outputs=micro_exp_output)\n",
    "    \n",
    "    return micro_exp_spatial_feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6074e1bea923d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape = (64,64,3)\n",
    "# inputs = Input(input_shape)\n",
    "# \n",
    "# # Layer 1\n",
    "# x = Conv2D(32,(3,3),padding='same')(inputs)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation('relu')(x)\n",
    "# x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "# \n",
    "# # Layer 2\n",
    "# x = Conv2D(64,(3,3),padding='same')(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation('relu')(x)\n",
    "# x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "# \n",
    "# # Layer 3\n",
    "# x = Conv2D(128,(3,3),padding='same')(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation('relu')(x)\n",
    "# x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "# \n",
    "# # Layer 4\n",
    "# x = Flatten()(x)\n",
    "# x = Dense(256, activation='relu')(x)\n",
    "# # Add dropouts in case of overfitting\n",
    "# \n",
    "# output = Dense(128, activation='relu')(x)\n",
    "# \n",
    "# micro_exp_feature_extractor = Model(inputs=inputs, outputs=output)\n",
    "# micro_exp_feature_extractor.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "# micro_exp_feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc7e9ddfed94684",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_exp_spatial = build_micro_exp_spatial_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b993d306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor shape=(None, 128), dtype=float32, sparse=False, name=keras_tensor_194>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "micro_exp_spatial.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc25588dbec308d",
   "metadata": {},
   "source": [
    "### **2. Temporal Inconsistency Detection in Micro Expression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e8d6e0b07c3d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.api.layers import Attention,Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5e07911f8822c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_temporal_inconsistency_attention():\n",
    "    temp_inputs = Input(shape=(30,128))\n",
    "    \n",
    "    x_mic_exp = Bidirectional(LSTM(128, return_sequences=True, dropout=0.2))(temp_inputs)\n",
    "    \n",
    "    x_mic_exp = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2))(x_mic_exp)\n",
    "    \n",
    "    attention_output = Attention()([x_mic_exp,x_mic_exp])\n",
    "    \n",
    "    x_mic_exp = Dense(256, activation='relu')(attention_output)\n",
    "    # Add dropout layers for overfitting\n",
    "    x_mic_exp = Dense(128, activation='relu')(x_mic_exp)\n",
    "    \n",
    "    mic_exp_temp_model = Model(inputs=temp_inputs, outputs=x_mic_exp)\n",
    "    \n",
    "    mic_exp_temp_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    \n",
    "    return mic_exp_temp_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c14bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_exp_temp = build_temporal_inconsistency_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bfba268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 30, 128)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mic_exp_temp.output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d43392f9d13e1a",
   "metadata": {},
   "source": [
    "## **3. Feature Fusion Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84699328876087a",
   "metadata": {},
   "source": [
    "### **1. Spatial Attention Mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48e2125903b88b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.api.layers import Multiply, GlobalAvgPool1D, GlobalAveragePooling2D, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334fc13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8122e940e8373bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_spatial_attention_mechanism(feature_maps):\n",
    "    \"\"\"\n",
    "    :param feature_maps: \n",
    "    :return: weighted feature maps\n",
    "    \"\"\"\n",
    "\n",
    "    expanded_tensor = Lambda(lambda x: tf.expand_dims(tf.expand_dims(x, axis=1),axis=2))(feature_maps)\n",
    "    \n",
    "    attention_map = Conv2D(1, kernel_size=(1,1), strides=(1,1), padding='same')(expanded_tensor)\n",
    "    \n",
    "    attention_map = Activation('sigmoid')(attention_map) # 'sigmoid' or 'softmax' can be used as an activation function\n",
    "    \n",
    "    # Element wise multiplication of feature_maps and attention_map\n",
    "    weighted_feature_map = Multiply()([feature_maps, attention_map])\n",
    "    \n",
    "    # Convert the weighted feature map into a context vector\n",
    "    spatial_context_vectors = GlobalAveragePooling2D()(expanded_tensor)\n",
    "    \n",
    "    return spatial_context_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92292e08c44b7b7b",
   "metadata": {},
   "source": [
    "### **2. Temporal Attention Mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49a9176a1176c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_temporal_attention_mechanism(feature_maps):\n",
    "    \"\"\"\n",
    "    :param feature_maps: \n",
    "    :return weighted_feature_maps: \n",
    "    \"\"\"\n",
    "    \n",
    "    temporal_attention_scores = Dense(1, activation='tanh')(feature_maps)\n",
    "    \n",
    "    temporal_attention_weights = Activation('softmax')(temporal_attention_scores)\n",
    "    \n",
    "    weighted_temporal_features = Multiply()([feature_maps, temporal_attention_weights])\n",
    "    \n",
    "    context_vector = Lambda(lambda x: tf.reduce_sum(x, axis=1))(weighted_temporal_features)\n",
    "    \n",
    "    return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad6d4050af9efff",
   "metadata": {},
   "source": [
    "### **3. Micro Expression Attention Mechanism**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6df99f573ee5e2c",
   "metadata": {},
   "source": [
    "#### **1. Spatial Micro Expression Attention Mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f16ab0296575dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_spatial_micro_expression_attention_mechanism(micro_exp_spatial_feature_maps):\n",
    "    \"\"\"\n",
    "    :param micro_exp_spatial_feature_maps: \n",
    "    :return weighted micro_exp feature maps : \n",
    "    \"\"\"\n",
    "\n",
    "    reshaped_map = Lambda(lambda x: tf.expand_dims(tf.expand_dims(x, axis=1),axis=2))(micro_exp_spatial_feature_maps)\n",
    "    \n",
    "    attention_map = Conv2D(1,(1,1),strides=(1,1),padding=\"same\")(reshaped_map)\n",
    "    \n",
    "    attention_map = Activation('sigmoid')(attention_map)\n",
    "    \n",
    "    weighted_micro_exp_feature_map = Multiply()([micro_exp_spatial_feature_maps,attention_map])\n",
    "    \n",
    "    micro_exp_spatial_context_vector = GlobalAveragePooling2D()(weighted_micro_exp_feature_map)\n",
    "    \n",
    "    return micro_exp_spatial_context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712249ac1c4655a1",
   "metadata": {},
   "source": [
    "#### **2. Temporal Micro Expression Attention Mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d70c9acf70368e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_temporal_micro_expression_attention_mechanism(micro_exp_feature_vectors):\n",
    "    \"\"\"\n",
    "    :param micro_exp_feature_vectors: \n",
    "    :return micro_exp_context_vectors: \n",
    "    \"\"\"\n",
    "    \n",
    "    attention_scores = Dense(1,activation='tanh')(micro_exp_feature_vectors)\n",
    "    \n",
    "    attention_weights = Activation('softmax')(attention_scores)\n",
    "    \n",
    "    weighted_micro_exp_temporal_features = Multiply()([attention_weights, micro_exp_feature_vectors])\n",
    "    \n",
    "    micro_exp_context_vector = Lambda(lambda x:tf.reduce_sum(x, axis=1))(weighted_micro_exp_temporal_features)\n",
    "    \n",
    "    return micro_exp_context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dec02a110a1397",
   "metadata": {},
   "source": [
    "### **4. Concatenation Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "286bd7dea56315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.api.layers import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23573323f25ea3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_fusion_layer(spatial_features, temporal_features, micro_exp_spatial_features, micro_exp_temporal_features):\n",
    "    \n",
    "    spatial_context_vectors = build_spatial_attention_mechanism(feature_maps=spatial_features)\n",
    "\n",
    "    temporal_context_vector = build_temporal_attention_mechanism(feature_maps=temporal_features)\n",
    "    \n",
    "    micro_exp_spatial_context_vector = build_spatial_micro_expression_attention_mechanism(micro_exp_spatial_feature_maps=micro_exp_spatial_features)\n",
    "    \n",
    "    micro_exp_temporal_context_vector = build_temporal_micro_expression_attention_mechanism(micro_exp_feature_vectors=micro_exp_temporal_features)\n",
    "    \n",
    "    concatenated_feature_vector = Concatenate()([\n",
    "        spatial_context_vectors,\n",
    "        temporal_context_vector,\n",
    "        micro_exp_spatial_context_vector,\n",
    "        micro_exp_temporal_context_vector\n",
    "    ])\n",
    "    \n",
    "    return concatenated_feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba6f7f72890a65",
   "metadata": {},
   "source": [
    "## **4. Face Swap Detection Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fe449d4d7cabf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_face_swap_detection_model(concatenated_feature_vector):\n",
    "    \n",
    "    dense_units = [256,128,64]\n",
    "    \n",
    "    x_face_swap = concatenated_feature_vector\n",
    "    for unit in dense_units:\n",
    "        x_face_swap = Dense(unit, activation='relu')(x_face_swap)\n",
    "        x_face_swap = Dropout(0.5)(x_face_swap)\n",
    "    \n",
    "    op_face_swap = Dense(1,activation='sigmoid')(x_face_swap)\n",
    "    \n",
    "    face_swap_detector_model = Model(inputs=concatenated_feature_vector, outputs=op_face_swap)\n",
    "    \n",
    "    return face_swap_detector_model    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b634cffad470773",
   "metadata": {},
   "source": [
    "## **Face Swap Detection Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2349e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_input = Input(shape=(30,2048))\n",
    "micro_exp_spatial_input = build_micro_exp_spatial_feature_extractor().input\n",
    "micro_exp_temporal_input = Input(shape=(30,128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7871a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:192: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spatial_features = spatial_model.output\n",
    "\n",
    "expanded_spatial_features = Lambda(lambda x: tf.expand_dims(x, axis=1))(spatial_features)\n",
    "reshaped_spatial_features = Lambda(lambda x: tf.reshape(x, (-1, 30, 2048)), output_shape=(30, 2048))(expanded_spatial_features)\n",
    "\n",
    "temporal_features = build_temporal_feature_extractor()(reshaped_spatial_features)\n",
    "\n",
    "micro_exp_spatial_features = build_micro_exp_spatial_feature_extractor()(micro_exp_spatial_input)\n",
    "\n",
    "expanded_micro_spatial_features = Lambda(lambda x: tf.expand_dims(x, axis=1))(micro_exp_spatial_features)\n",
    "reshaped_micro_spatial_features = Lambda(lambda x: tf.reshape(x, (-1, 30, 128)), output_shape=(30, 128))(expanded_micro_spatial_features)\n",
    "\n",
    "micro_exp_temporal_features = build_temporal_inconsistency_attention()(reshaped_micro_spatial_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51d368d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_features = build_feature_fusion_layer(\n",
    "    spatial_features, \n",
    "    temporal_features, \n",
    "    micro_exp_spatial_features,\n",
    "    micro_exp_temporal_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bcdcedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = build_face_swap_detection_model(fused_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66addb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from function import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "599cf87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_data_test1 = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a51e10dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_data_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db520f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_dataframe = pd.DataFrame(video_data_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f85411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_dataframe.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55bbe2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_dataframe = video_dataframe.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1298eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff0d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe882c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(video_dataframe['frames'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3c46ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_dataframe.index.name = \"video_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afa0fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5256fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #def assign_video_label(frame_labels, micro_expression_labels):\n",
    "#     # Any frame or micro-expression being manipulated sets video as manipulated (1)\n",
    "#     if np.any(np.array(frame_labels) == 1) or np.any(np.array(micro_expression_labels) == 1):\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8aaeaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_dataframe['video_label'] = video_dataframe.apply(\n",
    "#     lambda row: assign_video_label(row['frame_label'], row['Micro_Expression_label']),\n",
    "#     axis=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f1bba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_dataframe.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8943558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(video_dataframe['frames'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9333590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import pandas as pd\n",
    "# from PIL import Image\n",
    "# from io import BytesIO\n",
    "\n",
    "# # Function to convert PIL images to byte arrays\n",
    "# def pil_to_bytes(pil_img):\n",
    "#     with BytesIO() as buffer:\n",
    "#         pil_img.save(buffer, format='JPEG')\n",
    "#         return buffer.getvalue()\n",
    "\n",
    "# # Create a dictionary to hold the video data\n",
    "# pickled_data = {}\n",
    "\n",
    "# # Iterate over DataFrame rows\n",
    "# for idx, row in video_dataframe.iterrows():\n",
    "#     video_name = idx  # or another unique identifier if applicable\n",
    "#     pickled_data[video_name] = {\n",
    "#         'frames': [pil_to_bytes(img) for img in row['frames']],\n",
    "#         'frame_label': row['frame_label'],\n",
    "#         'Micro_Expression': [pil_to_bytes(img) for img in row['Micro_Expression']],\n",
    "#         'Micro_Expression_label': row['Micro_Expression_label']\n",
    "#     }\n",
    "\n",
    "# # Save the dictionary to a pickle file\n",
    "# with open('video_data_2.pkl', 'wb') as f:\n",
    "#     pickle.dump(pickled_data, f)\n",
    "\n",
    "# print(\"Data saved to pickle format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "542e245f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and restored.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "\n",
    "# Function to convert byte arrays back to PIL images\n",
    "def bytes_to_pil(byte_data):\n",
    "    with BytesIO(byte_data) as buffer:\n",
    "        return Image.open(buffer)\n",
    "\n",
    "# Load data from pickle file\n",
    "with open('video_data_2.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "# Create a list to hold the restored data\n",
    "restored_data = []\n",
    "\n",
    "# Reconstruct the DataFrame-like structure\n",
    "for video_name, video_info in loaded_data.items():\n",
    "    restored_data.append({\n",
    "        'video_name': video_name,\n",
    "        'frames': [bytes_to_pil(img_bytes) for img_bytes in video_info['frames']],\n",
    "        'frame_label': video_info['frame_label'],\n",
    "        'Micro_Expression': [bytes_to_pil(img_bytes) for img_bytes in video_info['Micro_Expression']],\n",
    "        'Micro_Expression_label': video_info['Micro_Expression_label']\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "restored_dataframe = pd.DataFrame(restored_data)\n",
    "\n",
    "print(\"Data loaded and restored.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e34fa005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.JpegImagePlugin.JpegImageFile"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(restored_dataframe['frames'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d30b26c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Compile the model\n",
    "final_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy']) # Adjust epochs as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "acae834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datamaker import VideoDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a790edfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the data from the pickle file\n",
    "with open('video_data_2.pkl', 'rb') as f:\n",
    "    pickled_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15d4c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert your video_data dictionary to a list of items for easier splitting\n",
    "data_items = list(pickled_data.items())\n",
    "video_names, labels = zip(*[(video_name, video_info['frame_label'][0]) for video_name, video_info in pickled_data.items()])\n",
    "\n",
    "# Split the data\n",
    "train_names, temp_names, train_labels, temp_labels = train_test_split(video_names, labels, test_size=0.3, random_state=42)\n",
    "val_names, test_names, val_labels, test_labels = train_test_split(temp_names, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Prepare dictionaries for each split\n",
    "train_data = {name: pickled_data[name] for name in train_names}\n",
    "val_data = {name: pickled_data[name] for name in val_names}\n",
    "test_data = {name: pickled_data[name] for name in test_names}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17fa8e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output signature for the generator\n",
    "output_signature = (\n",
    "    (\n",
    "        tf.TensorSpec(shape=(None, 30, 224, 224, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 30, 64, 64, 3), dtype=tf.float32)\n",
    "    ),\n",
    "    tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b21b4432",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = tf.data.Dataset.from_generator(\n",
    "    lambda: VideoDataGenerator(train_data),\n",
    "    output_signature=output_signature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eafa2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator = tf.data.Dataset.from_generator(\n",
    "    lambda: VideoDataGenerator(val_data),\n",
    "    output_signature=output_signature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d39bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = tf.data.Dataset.from_generator(\n",
    "    lambda: VideoDataGenerator(test_data),\n",
    "    output_signature=output_signature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6e18b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "facial_input = Input(shape=(30, 224, 224, 3), name='frames_input')\n",
    "micro_exp_inputs = Input(shape=(30, 64, 64, 3), name='micro_exp_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9cc1023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the final model\n",
    "dfds_model = Model(inputs=[facial_input, micro_exp_inputs], outputs=final_model.output)\n",
    "\n",
    "# Compile the model\n",
    "dfds_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0b42713e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None, 30, 224, 224, 3), (None, 30, 64, 64, 3)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfds_model.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "182aba12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_FlatMapDataset element_spec=((TensorSpec(shape=(None, 30, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 30, 64, 64, 3), dtype=tf.float32, name=None)), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0894e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8a7d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datamaker import VideoDataGenerator\n",
    "import tensorflow as tf\n",
    "from deliver import deliver_model\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "with open('../data/video_data_2.pkl', 'rb') as f:\n",
    "    pickled_data = pickle.load(f)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert your video_data dictionary to a list of items for easier splitting\n",
    "data_items = list(pickled_data.items())\n",
    "video_names, labels = zip(*[(video_name, video_info['frame_label'][0]) for video_name, video_info in pickled_data.items()])\n",
    "\n",
    "# Split the data\n",
    "train_names, temp_names, train_labels, temp_labels = train_test_split(video_names, labels, test_size=0.3, random_state=42)\n",
    "val_names, test_names, val_labels, test_labels = train_test_split(temp_names, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Prepare dictionaries for each split\n",
    "train_data = {name: pickled_data[name] for name in train_names}\n",
    "val_data = {name: pickled_data[name] for name in val_names}\n",
    "test_data = {name: pickled_data[name] for name in test_names}\n",
    "\n",
    "# Define the output signature for the generator\n",
    "output_signature = (\n",
    "    (\n",
    "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32)\n",
    "    ),\n",
    "    tf.TensorSpec(shape=(None,1), dtype=tf.float32)\n",
    ")\n",
    "\n",
    "train_generator = tf.data.Dataset.from_generator(\n",
    "    lambda: VideoDataGenerator(train_data),\n",
    "    output_signature=output_signature\n",
    ")\n",
    "\n",
    "val_generator = tf.data.Dataset.from_generator(\n",
    "    lambda: VideoDataGenerator(val_data),\n",
    "    output_signature=output_signature\n",
    ")\n",
    "\n",
    "test_generator = tf.data.Dataset.from_generator(\n",
    "    lambda: VideoDataGenerator(test_data),\n",
    "    output_signature=output_signature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe75c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deliver import deliver_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d76540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:192: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_test_1 = deliver_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d62ede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e33c930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,224,448</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ functional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ functional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,393,600</span> │ lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ functional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_3        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">493,440</span> │ lambda_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ functional_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ lambda_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ functional_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_4        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_5        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_6        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ functional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply_1          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ functional_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)          │                   │            │ activation_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply_2          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ functional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)          │                   │            │ activation_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply_3          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)          │                   │            │ functional_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2432</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lambda_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                     │                   │            │ global_average_p… │\n",
       "│                     │                   │            │ lambda_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">622,848</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │ \u001b[38;5;34m23,587,712\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │  \u001b[38;5;34m2,224,448\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2048\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ functional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ functional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m2048\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_3 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ lambda_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m2,393,600\u001b[0m │ lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_6 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ functional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_3        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m493,440\u001b[0m │ lambda_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │        \u001b[38;5;34m129\u001b[0m │ functional_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │        \u001b[38;5;34m129\u001b[0m │ lambda_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │        \u001b[38;5;34m129\u001b[0m │ functional_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_4        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_5        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ conv2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_6        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_4 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ functional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│                     │ \u001b[38;5;34m2048\u001b[0m)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply_1          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ functional_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMultiply\u001b[0m)          │                   │            │ activation_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply_2          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ functional_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMultiply\u001b[0m)          │                   │            │ activation_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply_3          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ activation_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mMultiply\u001b[0m)          │                   │            │ functional_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ lambda_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_5 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ multiply_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ multiply_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_7 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ multiply_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2432\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lambda_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                     │                   │            │ global_average_p… │\n",
       "│                     │                   │            │ lambda_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m622,848\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,363,652</span> (112.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m29,363,652\u001b[0m (112.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,775,492</span> (22.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,775,492\u001b[0m (22.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,588,160</span> (89.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,588,160\u001b[0m (89.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_test_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "066b0ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot get result() since the metric has not yet been built.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel_test_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\compile_utils.py:356\u001b[0m, in \u001b[0;36mCompileMetrics.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[1;32m--> 356\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    357\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get result() since the metric has not yet been built.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    358\u001b[0m         )\n\u001b[0;32m    359\u001b[0m     results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    360\u001b[0m     unique_name_counters \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot get result() since the metric has not yet been built."
     ]
    }
   ],
   "source": [
    "model_test_1.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c49646d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
