{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d427c0897c34fdf8",
   "metadata": {},
   "source": [
    "# Introduction to the Integrated Deepfake Detection System\n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to the Integrated Deepfake Detection System! This project is a comprehensive effort to tackle the growing problem of deepfake videos using advanced machine learning techniques. As deepfake technology continues to evolve, it poses significant challenges to privacy, security, and authenticity in media. Our system is designed to detect deepfakes by analyzing multiple aspects of video content, specifically focusing on spatial, temporal, and micro-expression features. By integrating these different feature types, we aim to create a robust and reliable detection mechanism capable of identifying manipulated video content.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The primary objective of this project is to develop a deepfake detection system that can accurately distinguish between genuine and manipulated videos. This involves:\n",
    "\n",
    "1. **Spatial Feature Extraction**: Analyzing individual frames of a video to capture static facial features. This is accomplished using pre-trained Convolutional Neural Networks (CNNs) such as ResNet50 and VGG16, which are fine-tuned for this task.\n",
    "   \n",
    "2. **Temporal Feature Extraction**: Understanding how facial features change over time by analyzing sequences of frames. Bidirectional Long Short-Term Memory (BiLSTM) networks are employed to capture temporal dependencies and detect inconsistencies in the flow of facial expressions.\n",
    "\n",
    "3. **Micro-Expression Analysis**: Focusing on subtle facial movements that are difficult to replicate in deepfake videos. This module uses specialized CNN architectures to extract and analyze micro-expressions, providing an additional layer of detection.\n",
    "\n",
    "4. **Feature Fusion**: Combining spatial, temporal, and micro-expression features using attention mechanisms to form a comprehensive feature set that enhances detection accuracy. This fusion approach leverages the strengths of each feature type to make a final decision about the authenticity of the video.\n",
    "\n",
    "## Why This Approach?\n",
    "\n",
    "Deepfake detection is a challenging task due to the sophistication of the algorithms used to create these fakes. Traditional detection methods that rely on a single type of feature often fail to capture the complexity of manipulations. By integrating spatial, temporal, and micro-expression features, our approach provides a multi-dimensional analysis that significantly improves the likelihood of detecting deepfakes. This holistic strategy addresses the limitations of existing methods and provides a more reliable solution.\n",
    "\n",
    "### Key Challenges Addressed:\n",
    "\n",
    "- **Variability in Deepfake Techniques**: Different deepfake algorithms have varying strengths and weaknesses. By analyzing multiple aspects of the video, our system can detect a wide range of manipulations.\n",
    "- **Subtle Manipulations**: Some deepfakes are so well-crafted that the manipulations are not immediately noticeable to the human eye. Micro-expression analysis helps in detecting these subtle manipulations.\n",
    "- **Generalization**: Ensuring that the model is not overfitting to specific datasets or types of deepfakes. The fusion of different feature types helps the model generalize better across diverse datasets.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "For this project, we use the **FaceForensics++** dataset, which is a benchmark dataset commonly used in deepfake detection research. It contains a collection of both original and manipulated video sequences, providing a diverse set of examples for training and testing the system. The dataset is divided into two main categories:\n",
    "\n",
    "- **Original Sequences**: Videos that have not been altered, serving as ground truth for authenticity.\n",
    "- **Manipulated Sequences**: Videos that have been altered using various deepfake techniques, providing examples of fake content.\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "This Jupyter Notebook is structured to guide you through the different stages of the deepfake detection process:\n",
    "\n",
    "1. **Data Preprocessing**: Preparing the video frames for feature extraction, including face detection, alignment, and normalization.\n",
    "   \n",
    "2. **Model Architecture**: Detailed implementation of the spatial, temporal, and micro-expression feature extraction models. This section includes building and training the CNN (ResNet50, VGG16), BiLSTM, and micro-expression analysis models.\n",
    "\n",
    "3. **Feature Fusion and Classification**: Combining the extracted features and using attention mechanisms to improve the detection accuracy. The final output layer provides the classification result, indicating whether the video is genuine or a deepfake.\n",
    "\n",
    "4. **Evaluation and Results**: Testing the trained model on a set of test videos and evaluating its performance using metrics such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "5. **Conclusion and Future Work**: Summarizing the findings and discussing potential improvements and future directions for research in deepfake detection.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "To run this notebook, ensure that you have all the necessary dependencies installed. The required libraries are listed in the `requirements.txt` file. Use the following command to install them:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Additionally, make sure you have the FaceForensics++ dataset downloaded and placed in the appropriate directory as specified in the notebook.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The Integrated Deepfake Detection System represents a significant step towards addressing the challenges posed by deepfake technology. By leveraging multiple feature extraction methods and incorporating attention mechanisms, this system aims to provide a robust solution for identifying manipulated video content. We hope that this project will contribute to the ongoing efforts to maintain the integrity and authenticity of digital media.\n",
    "\n",
    "Let's get started and dive into the world of deepfake detection!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9540ebef66cb2a0",
   "metadata": {},
   "source": [
    "## **Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "id": "5c3172c61182ab4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T14:58:32.298153Z",
     "start_time": "2024-09-24T14:58:31.117239Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "b1b4580d4832d031",
   "metadata": {},
   "source": [
    "## **Preprocessing Phase**"
   ]
  },
  {
   "cell_type": "code",
   "id": "5da46b4e2841c322",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T14:58:32.304130Z",
     "start_time": "2024-09-24T14:58:32.298153Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "e7953a0c42f06ed",
   "metadata": {},
   "source": [
    "## **Model Building**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5196a483ec9cd14",
   "metadata": {},
   "source": [
    "## 1. **Facial Feature Extractor Module**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba19f2fcb81df0d0",
   "metadata": {},
   "source": [
    "### 1. Spatial Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-24T14:58:37.657052Z",
     "start_time": "2024-09-24T14:58:32.304130Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.api.layers import TimeDistributed,Input, Flatten, GlobalAvgPool1D\n",
    "from keras.api.applications.resnet50 import  ResNet50, preprocess_input as resnet_preprocess\n",
    "from keras.api.models import  Model\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TimeDistributed,Input, Flatten, GlobalAvgPool1D\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mresnet50\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m  ResNet50, preprocess_input \u001B[38;5;28;01mas\u001B[39;00m resnet_preprocess\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\__init__.py:47\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf2 \u001B[38;5;28;01mas\u001B[39;00m _tf2\n\u001B[0;32m     45\u001B[0m _tf2\u001B[38;5;241m.\u001B[39menable()\n\u001B[1;32m---> 47\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __internal__\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __operators__\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m audio\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:11\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m decorator\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dispatch\n\u001B[1;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m eager_context\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m feature_column\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py:8\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute namespace\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_sys\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m combinations\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m interim\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m multi_process_runner\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\combinations\\__init__.py:8\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute.combinations namespace\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_sys\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcombinations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m env \u001B[38;5;66;03m# line: 456\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcombinations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m generate \u001B[38;5;66;03m# line: 365\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcombinations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m in_main_process \u001B[38;5;66;03m# line: 418\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\distribute\\combinations.py:33\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msix\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclient\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m session\n\u001B[1;32m---> 33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m collective_all_reduce_strategy\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute_lib\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m multi_process_runner\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py:25\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tensorflow_server_pb2\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m collective_util\n\u001B[1;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cross_device_ops \u001B[38;5;28;01mas\u001B[39;00m cross_device_ops_lib\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cross_device_utils\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m device_util\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_ops.py:28\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclient\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m device_lib\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m collective_util\n\u001B[1;32m---> 28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cross_device_utils\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m device_util\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute_utils\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_utils.py:22\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Callable, List, Optional, Union\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m collective_util\n\u001B[1;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m values \u001B[38;5;28;01mas\u001B[39;00m value_lib\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backprop_util\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m context\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\distribute\\values.py:23\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m struct_pb2\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m device_util\n\u001B[1;32m---> 23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute_lib\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m packed_distributed_variable \u001B[38;5;28;01mas\u001B[39;00m packed\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m reduce_util\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:205\u001B[0m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograph\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ag_ctx \u001B[38;5;28;01mas\u001B[39;00m autograph_ctx\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograph\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimpl\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m api \u001B[38;5;28;01mas\u001B[39;00m autograph\n\u001B[1;32m--> 205\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dataset_ops\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m collective_util\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m device_util\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\__init__.py:21\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"`tf.data.Dataset` API for input pipelines.\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \n\u001B[0;32m     17\u001B[0m \u001B[38;5;124;03mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# pylint: disable=unused-import\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m experimental\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AUTOTUNE\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:98\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Experimental API for building input pipelines.\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \n\u001B[0;32m     17\u001B[0m \u001B[38;5;124;03mThis module contains experimental `Dataset` sources and transformations that can\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;124;03m@@UNKNOWN_CARDINALITY\u001B[39;00m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     97\u001B[0m \u001B[38;5;66;03m# pylint: disable=unused-import\u001B[39;00m\n\u001B[1;32m---> 98\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m service\n\u001B[0;32m     99\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbatching\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dense_to_ragged_batch\n\u001B[0;32m    100\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbatching\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dense_to_sparse_batch\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:419\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"API for using the tf.data service.\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \n\u001B[0;32m     17\u001B[0m \u001B[38;5;124;03mThis module contains:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    416\u001B[0m \u001B[38;5;124;03m  job of ParameterServerStrategy).\u001B[39;00m\n\u001B[0;32m    417\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m--> 419\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[0;32m    420\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m from_dataset_id\n\u001B[0;32m    421\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m register_dataset\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:26\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mservice\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _pywrap_server_lib\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mservice\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _pywrap_utils_exp\n\u001B[1;32m---> 26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dataset_ops\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m options \u001B[38;5;28;01mas\u001B[39;00m options_lib\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m structured_function\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:33\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m v2_compat\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dataset_autograph\n\u001B[1;32m---> 33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m debug_mode\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m iterator_ops\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m options \u001B[38;5;28;01mas\u001B[39;00m options_lib\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1360\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1322\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1262\u001B[0m, in \u001B[0;36m_find_spec\u001B[1;34m(name, path, target)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1528\u001B[0m, in \u001B[0;36mfind_spec\u001B[1;34m(cls, fullname, path, target)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1502\u001B[0m, in \u001B[0;36m_get_spec\u001B[1;34m(cls, fullname, path, target)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1601\u001B[0m, in \u001B[0;36mfind_spec\u001B[1;34m(self, fullname, target)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:147\u001B[0m, in \u001B[0;36m_path_stat\u001B[1;34m(path)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "7fe90f1b1c190f6e",
   "metadata": {},
   "source": [
    "base_model = ResNet50(include_top=False, weights='imagenet', pooling='avg', input_shape=(224,224,3))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dd898bcb7d806b01",
   "metadata": {},
   "source": [
    "spatial_model =  Model(inputs=base_model.input, outputs=base_model.output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "baa2dd11",
   "metadata": {},
   "source": [
    "for layer in spatial_model.layers:\n",
    "    layer.trainable = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5e6fb47a",
   "metadata": {},
   "source": [
    "spatial_model.output.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "36f30f05",
   "metadata": {},
   "source": [
    "### 2. Temporal Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "id": "2294abec",
   "metadata": {},
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Bidirectional,LSTM,Input\n",
    "from keras.models import Model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "601c81d8",
   "metadata": {},
   "source": [
    "def build_temporal_feature_extractor():\n",
    "    input_seq = Input(shape=(30,2048))\n",
    "\n",
    "    lstm_1 = LSTM(128, return_sequences=True, dropout=0.2)\n",
    "    lstm_2 = LSTM(64,return_sequences=True, dropout=0.2)\n",
    "\n",
    "    lstm_out = Bidirectional(lstm_1)(input_seq)\n",
    "    lstm_out = Bidirectional(lstm_2)(lstm_out)\n",
    "\n",
    "    model = Model(inputs=input_seq, outputs=lstm_out)\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4522e339",
   "metadata": {},
   "source": [
    "temporal_model = build_temporal_feature_extractor()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "246f329b",
   "metadata": {},
   "source": [
    "temporal_model.output"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4b8839ed419f1aec",
   "metadata": {},
   "source": [
    "## **2. Micro Expression Inconsistency Detection Module**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7483995b04b067",
   "metadata": {},
   "source": [
    "### **1. Micro Expression Feature Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "id": "8e395e308720792",
   "metadata": {},
   "source": [
    "from keras.api.layers import Conv2D,BatchNormalization,Activation,MaxPooling2D,Dropout, Dense, Flatten"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bffd388828a69382",
   "metadata": {},
   "source": [
    "def build_micro_exp_spatial_feature_extractor():\n",
    "    spatial_inputs = Input(shape=(64,64,3))\n",
    "\n",
    "    # Layer 1\n",
    "    micro_exp_x = Conv2D(32,(3,3),padding='same')(spatial_inputs)\n",
    "    micro_exp_x = BatchNormalization()(micro_exp_x)\n",
    "    micro_exp_x = Activation('relu')(micro_exp_x)\n",
    "    micro_exp_x = MaxPooling2D(pool_size=(2,2))(micro_exp_x)\n",
    "    \n",
    "    # Layer 2\n",
    "    micro_exp_x = Conv2D(64,(3,3),padding='same')(micro_exp_x)\n",
    "    micro_exp_x = BatchNormalization()(micro_exp_x)\n",
    "    micro_exp_x = Activation('relu')(micro_exp_x)\n",
    "    micro_exp_x = MaxPooling2D(pool_size=(2,2))(micro_exp_x)\n",
    "    \n",
    "    # Layer 3\n",
    "    micro_exp_x = Conv2D(128,(3,3),padding='same')(micro_exp_x)\n",
    "    micro_exp_x = BatchNormalization()(micro_exp_x)\n",
    "    micro_exp_x = Activation('relu')(micro_exp_x)\n",
    "    micro_exp_x = MaxPooling2D(pool_size=(2,2))(micro_exp_x)\n",
    "    \n",
    "    # Layer 4\n",
    "    micro_exp_x = Flatten()(micro_exp_x)\n",
    "    micro_exp_x = Dense(256, activation='relu')(micro_exp_x)\n",
    "    # Add dropouts in case of overfitting\n",
    "    \n",
    "    micro_exp_output = Dense(128, activation='relu')(micro_exp_x)\n",
    "    \n",
    "    micro_exp_spatial_feature_extractor = Model(inputs=spatial_inputs, outputs=micro_exp_output)\n",
    "    \n",
    "    return micro_exp_spatial_feature_extractor"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6074e1bea923d5f4",
   "metadata": {},
   "source": [
    "# input_shape = (64,64,3)\n",
    "# inputs = Input(input_shape)\n",
    "# \n",
    "# # Layer 1\n",
    "# x = Conv2D(32,(3,3),padding='same')(inputs)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation('relu')(x)\n",
    "# x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "# \n",
    "# # Layer 2\n",
    "# x = Conv2D(64,(3,3),padding='same')(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation('relu')(x)\n",
    "# x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "# \n",
    "# # Layer 3\n",
    "# x = Conv2D(128,(3,3),padding='same')(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation('relu')(x)\n",
    "# x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "# \n",
    "# # Layer 4\n",
    "# x = Flatten()(x)\n",
    "# x = Dense(256, activation='relu')(x)\n",
    "# # Add dropouts in case of overfitting\n",
    "# \n",
    "# output = Dense(128, activation='relu')(x)\n",
    "# \n",
    "# micro_exp_feature_extractor = Model(inputs=inputs, outputs=output)\n",
    "# micro_exp_feature_extractor.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "# micro_exp_feature_extractor.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc7e9ddfed94684",
   "metadata": {},
   "source": [
    "micro_exp_spatial = build_micro_exp_spatial_feature_extractor()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b993d306",
   "metadata": {},
   "source": [
    "micro_exp_spatial.output"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bbc25588dbec308d",
   "metadata": {},
   "source": [
    "### **2. Temporal Inconsistency Detection in Micro Expression**"
   ]
  },
  {
   "cell_type": "code",
   "id": "5e8d6e0b07c3d7cc",
   "metadata": {},
   "source": [
    "from keras.api.layers import Attention,Concatenate"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5e07911f8822c6",
   "metadata": {},
   "source": [
    "def build_temporal_inconsistency_attention():\n",
    "    temp_inputs = Input(shape=(30,128))\n",
    "    \n",
    "    x_mic_exp = Bidirectional(LSTM(128, return_sequences=True, dropout=0.2))(temp_inputs)\n",
    "    \n",
    "    x_mic_exp = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2))(x_mic_exp)\n",
    "    \n",
    "    attention_output = Attention()([x_mic_exp,x_mic_exp])\n",
    "    \n",
    "    x_mic_exp = Dense(256, activation='relu')(attention_output)\n",
    "    # Add dropout layers for overfitting\n",
    "    x_mic_exp = Dense(128, activation='relu')(x_mic_exp)\n",
    "    \n",
    "    mic_exp_temp_model = Model(inputs=temp_inputs, outputs=x_mic_exp)\n",
    "    \n",
    "    mic_exp_temp_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    \n",
    "    return mic_exp_temp_model\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2c14bb45",
   "metadata": {},
   "source": [
    "mic_exp_temp = build_temporal_inconsistency_attention()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4bfba268",
   "metadata": {},
   "source": [
    "mic_exp_temp.output.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "57d43392f9d13e1a",
   "metadata": {},
   "source": [
    "## **3. Feature Fusion Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84699328876087a",
   "metadata": {},
   "source": [
    "### **1. Spatial Attention Mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "id": "48e2125903b88b25",
   "metadata": {},
   "source": [
    "from keras.api.layers import Multiply, GlobalAvgPool1D, GlobalAveragePooling2D, Lambda"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "334fc13a",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8122e940e8373bf0",
   "metadata": {},
   "source": [
    "def build_spatial_attention_mechanism(feature_maps):\n",
    "    \"\"\"\n",
    "    :param feature_maps: \n",
    "    :return: weighted feature maps\n",
    "    \"\"\"\n",
    "\n",
    "    expanded_tensor = Lambda(lambda x: tf.expand_dims(tf.expand_dims(x, axis=1),axis=2))(feature_maps)\n",
    "    \n",
    "    attention_map = Conv2D(1, kernel_size=(1,1), strides=(1,1), padding='same')(expanded_tensor)\n",
    "    \n",
    "    attention_map = Activation('sigmoid')(attention_map) # 'sigmoid' or 'softmax' can be used as an activation function\n",
    "    \n",
    "    # Element wise multiplication of feature_maps and attention_map\n",
    "    weighted_feature_map = Multiply()([feature_maps, attention_map])\n",
    "    \n",
    "    # Convert the weighted feature map into a context vector\n",
    "    spatial_context_vectors = GlobalAveragePooling2D()(expanded_tensor)\n",
    "    \n",
    "    return spatial_context_vectors\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "92292e08c44b7b7b",
   "metadata": {},
   "source": [
    "### **2. Temporal Attention Mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "id": "49a9176a1176c8c3",
   "metadata": {},
   "source": [
    "def build_temporal_attention_mechanism(feature_maps):\n",
    "    \"\"\"\n",
    "    :param feature_maps: \n",
    "    :return weighted_feature_maps: \n",
    "    \"\"\"\n",
    "    \n",
    "    temporal_attention_scores = Dense(1, activation='tanh')(feature_maps)\n",
    "    \n",
    "    temporal_attention_weights = Activation('softmax')(temporal_attention_scores)\n",
    "    \n",
    "    weighted_temporal_features = Multiply()([feature_maps, temporal_attention_weights])\n",
    "    \n",
    "    context_vector = Lambda(lambda x: tf.reduce_sum(x, axis=1))(weighted_temporal_features)\n",
    "    \n",
    "    return context_vector"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aad6d4050af9efff",
   "metadata": {},
   "source": [
    "### **3. Micro Expression Attention Mechanism**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6df99f573ee5e2c",
   "metadata": {},
   "source": [
    "#### **1. Spatial Micro Expression Attention Mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "id": "f16ab0296575dd91",
   "metadata": {},
   "source": [
    "def build_spatial_micro_expression_attention_mechanism(micro_exp_spatial_feature_maps):\n",
    "    \"\"\"\n",
    "    :param micro_exp_spatial_feature_maps: \n",
    "    :return weighted micro_exp feature maps : \n",
    "    \"\"\"\n",
    "\n",
    "    reshaped_map = Lambda(lambda x: tf.expand_dims(tf.expand_dims(x, axis=1),axis=2))(micro_exp_spatial_feature_maps)\n",
    "    \n",
    "    attention_map = Conv2D(1,(1,1),strides=(1,1),padding=\"same\")(reshaped_map)\n",
    "    \n",
    "    attention_map = Activation('sigmoid')(attention_map)\n",
    "    \n",
    "    weighted_micro_exp_feature_map = Multiply()([micro_exp_spatial_feature_maps,attention_map])\n",
    "    \n",
    "    micro_exp_spatial_context_vector = GlobalAveragePooling2D()(weighted_micro_exp_feature_map)\n",
    "    \n",
    "    return micro_exp_spatial_context_vector"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "712249ac1c4655a1",
   "metadata": {},
   "source": [
    "#### **2. Temporal Micro Expression Attention Mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "id": "3d70c9acf70368e3",
   "metadata": {},
   "source": [
    "def build_temporal_micro_expression_attention_mechanism(micro_exp_feature_vectors):\n",
    "    \"\"\"\n",
    "    :param micro_exp_feature_vectors: \n",
    "    :return micro_exp_context_vectors: \n",
    "    \"\"\"\n",
    "    \n",
    "    attention_scores = Dense(1,activation='tanh')(micro_exp_feature_vectors)\n",
    "    \n",
    "    attention_weights = Activation('softmax')(attention_scores)\n",
    "    \n",
    "    weighted_micro_exp_temporal_features = Multiply()([attention_weights, micro_exp_feature_vectors])\n",
    "    \n",
    "    micro_exp_context_vector = Lambda(lambda x:tf.reduce_sum(x, axis=1))(weighted_micro_exp_temporal_features)\n",
    "    \n",
    "    return micro_exp_context_vector"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "85dec02a110a1397",
   "metadata": {},
   "source": [
    "### **4. Concatenation Layer**"
   ]
  },
  {
   "cell_type": "code",
   "id": "286bd7dea56315f",
   "metadata": {},
   "source": [
    "from keras.api.layers import Concatenate"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "23573323f25ea3f8",
   "metadata": {},
   "source": [
    "def build_feature_fusion_layer(spatial_features, temporal_features, micro_exp_spatial_features, micro_exp_temporal_features):\n",
    "    \n",
    "    spatial_context_vectors = build_spatial_attention_mechanism(feature_maps=spatial_features)\n",
    "\n",
    "    temporal_context_vector = build_temporal_attention_mechanism(feature_maps=temporal_features)\n",
    "    \n",
    "    micro_exp_spatial_context_vector = build_spatial_micro_expression_attention_mechanism(micro_exp_spatial_feature_maps=micro_exp_spatial_features)\n",
    "    \n",
    "    micro_exp_temporal_context_vector = build_temporal_micro_expression_attention_mechanism(micro_exp_feature_vectors=micro_exp_temporal_features)\n",
    "    \n",
    "    concatenated_feature_vector = Concatenate()([\n",
    "        spatial_context_vectors,\n",
    "        temporal_context_vector,\n",
    "        micro_exp_spatial_context_vector,\n",
    "        micro_exp_temporal_context_vector\n",
    "    ])\n",
    "    \n",
    "    return concatenated_feature_vector"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aaba6f7f72890a65",
   "metadata": {},
   "source": [
    "## **4. Face Swap Detection Model**"
   ]
  },
  {
   "cell_type": "code",
   "id": "9fe449d4d7cabf57",
   "metadata": {},
   "source": [
    "def build_face_swap_detection_model(concatenated_feature_vector):\n",
    "    \n",
    "    dense_units = [256,128,64]\n",
    "    \n",
    "    x_face_swap = concatenated_feature_vector\n",
    "    for unit in dense_units:\n",
    "        x_face_swap = Dense(unit, activation='relu')(x_face_swap)\n",
    "        x_face_swap = Dropout(0.5)(x_face_swap)\n",
    "    \n",
    "    op_face_swap = Dense(1,activation='sigmoid')(x_face_swap)\n",
    "    \n",
    "    face_swap_detector_model = Model(inputs=concatenated_feature_vector, outputs=op_face_swap)\n",
    "    \n",
    "    return face_swap_detector_model    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5b634cffad470773",
   "metadata": {},
   "source": [
    "## **Face Swap Detection Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "id": "2349e9cb",
   "metadata": {},
   "source": [
    "temporal_input = Input(shape=(30,2048))\n",
    "micro_exp_spatial_input = build_micro_exp_spatial_feature_extractor().input\n",
    "micro_exp_temporal_input = Input(shape=(30,128))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c7871a64",
   "metadata": {},
   "source": [
    "spatial_features = spatial_model.output\n",
    "\n",
    "expanded_spatial_features = Lambda(lambda x: tf.expand_dims(x, axis=1))(spatial_features)\n",
    "reshaped_spatial_features = Lambda(lambda x: tf.reshape(x, (-1, 30, 2048)), output_shape=(30, 2048))(expanded_spatial_features)\n",
    "\n",
    "temporal_features = build_temporal_feature_extractor()(reshaped_spatial_features)\n",
    "\n",
    "micro_exp_spatial_features = build_micro_exp_spatial_feature_extractor()(micro_exp_spatial_input)\n",
    "\n",
    "expanded_micro_spatial_features = Lambda(lambda x: tf.expand_dims(x, axis=1))(micro_exp_spatial_features)\n",
    "reshaped_micro_spatial_features = Lambda(lambda x: tf.reshape(x, (-1, 30, 128)), output_shape=(30, 128))(expanded_micro_spatial_features)\n",
    "\n",
    "micro_exp_temporal_features = build_temporal_inconsistency_attention()(reshaped_micro_spatial_features)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "51d368d6",
   "metadata": {},
   "source": [
    "fused_features = build_feature_fusion_layer(\n",
    "    spatial_features, \n",
    "    temporal_features, \n",
    "    micro_exp_spatial_features,\n",
    "    micro_exp_temporal_features\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9bcdcedb",
   "metadata": {},
   "source": [
    "final_model = build_face_swap_detection_model(fused_features)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "66addb91",
   "metadata": {},
   "source": [
    "#from function import load_dataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "599cf87a",
   "metadata": {},
   "source": [
    "#video_data_test1 = load_dataset()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a51e10dc",
   "metadata": {},
   "source": [
    "#video_data_test1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "db520f22",
   "metadata": {},
   "source": [
    "#video_dataframe = pd.DataFrame(video_data_test1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f85411d",
   "metadata": {},
   "source": [
    "#video_dataframe.head(5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "55bbe2e2",
   "metadata": {},
   "source": [
    "#video_dataframe = video_dataframe.transpose()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1298eb05",
   "metadata": {},
   "source": [
    "#video_dataframe.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40ff0d8f",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe882c6d",
   "metadata": {},
   "source": [
    "#type(video_dataframe['frames'][0][0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b3c46ad0",
   "metadata": {},
   "source": [
    "#video_dataframe.index.name = \"video_index\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "afa0fef9",
   "metadata": {},
   "source": [
    "#video_dataframe"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5256fe1",
   "metadata": {},
   "source": [
    "# #def assign_video_label(frame_labels, micro_expression_labels):\n",
    "#     # Any frame or micro-expression being manipulated sets video as manipulated (1)\n",
    "#     if np.any(np.array(frame_labels) == 1) or np.any(np.array(micro_expression_labels) == 1):\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8aaeaae",
   "metadata": {},
   "source": [
    "# video_dataframe['video_label'] = video_dataframe.apply(\n",
    "#     lambda row: assign_video_label(row['frame_label'], row['Micro_Expression_label']),\n",
    "#     axis=1\n",
    "# )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f1bba11",
   "metadata": {},
   "source": [
    "# video_dataframe.head(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b8943558",
   "metadata": {},
   "source": [
    "# type(video_dataframe['frames'][0][0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9333590a",
   "metadata": {},
   "source": [
    "# import pickle\n",
    "# import pandas as pd\n",
    "# from PIL import Image\n",
    "# from io import BytesIO\n",
    "\n",
    "# # Function to convert PIL images to byte arrays\n",
    "# def pil_to_bytes(pil_img):\n",
    "#     with BytesIO() as buffer:\n",
    "#         pil_img.save(buffer, format='JPEG')\n",
    "#         return buffer.getvalue()\n",
    "\n",
    "# # Create a dictionary to hold the video data\n",
    "# pickled_data = {}\n",
    "\n",
    "# # Iterate over DataFrame rows\n",
    "# for idx, row in video_dataframe.iterrows():\n",
    "#     video_name = idx  # or another unique identifier if applicable\n",
    "#     pickled_data[video_name] = {\n",
    "#         'frames': [pil_to_bytes(img) for img in row['frames']],\n",
    "#         'frame_label': row['frame_label'],\n",
    "#         'Micro_Expression': [pil_to_bytes(img) for img in row['Micro_Expression']],\n",
    "#         'Micro_Expression_label': row['Micro_Expression_label']\n",
    "#     }\n",
    "\n",
    "# # Save the dictionary to a pickle file\n",
    "# with open('video_data_2.pkl', 'wb') as f:\n",
    "#     pickle.dump(pickled_data, f)\n",
    "\n",
    "# print(\"Data saved to pickle format.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "542e245f",
   "metadata": {},
   "source": [
    "import pickle\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "\n",
    "# Function to convert byte arrays back to PIL images\n",
    "def bytes_to_pil(byte_data):\n",
    "    with BytesIO(byte_data) as buffer:\n",
    "        return Image.open(buffer)\n",
    "\n",
    "# Load data from pickle file\n",
    "with open('video_data_2.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "# Create a list to hold the restored data\n",
    "restored_data = []\n",
    "\n",
    "# Reconstruct the DataFrame-like structure\n",
    "for video_name, video_info in loaded_data.items():\n",
    "    restored_data.append({\n",
    "        'video_name': video_name,\n",
    "        'frames': [bytes_to_pil(img_bytes) for img_bytes in video_info['frames']],\n",
    "        'frame_label': video_info['frame_label'],\n",
    "        'Micro_Expression': [bytes_to_pil(img_bytes) for img_bytes in video_info['Micro_Expression']],\n",
    "        'Micro_Expression_label': video_info['Micro_Expression_label']\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "restored_dataframe = pd.DataFrame(restored_data)\n",
    "\n",
    "print(\"Data loaded and restored.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e34fa005",
   "metadata": {},
   "source": [
    "type(restored_dataframe['frames'][0][0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d30b26c4",
   "metadata": {},
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Compile the model\n",
    "final_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy']) # Adjust epochs as needed\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "acae834b",
   "metadata": {},
   "source": [
    "from datamaker import VideoDataGenerator"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a790edfd",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "# Load the data from the pickle file\n",
    "with open('video_data_2.pkl', 'rb') as f:\n",
    "    pickled_data = pickle.load(f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15d4c449",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert your video_data dictionary to a list of items for easier splitting\n",
    "data_items = list(pickled_data.items())\n",
    "video_names, labels = zip(*[(video_name, video_info['frame_label'][0]) for video_name, video_info in pickled_data.items()])\n",
    "\n",
    "# Split the data\n",
    "train_names, temp_names, train_labels, temp_labels = train_test_split(video_names, labels, test_size=0.3, random_state=42)\n",
    "val_names, test_names, val_labels, test_labels = train_test_split(temp_names, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Prepare dictionaries for each split\n",
    "train_data = {name: pickled_data[name] for name in train_names}\n",
    "val_data = {name: pickled_data[name] for name in val_names}\n",
    "test_data = {name: pickled_data[name] for name in test_names}\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17fa8e6e",
   "metadata": {},
   "source": [
    "# Define the output signature for the generator\n",
    "output_signature = (\n",
    "    (\n",
    "        tf.TensorSpec(shape=(None, 30, 224, 224, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 30, 64, 64, 3), dtype=tf.float32)\n",
    "    ),\n",
    "    tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b21b4432",
   "metadata": {},
   "source": [
    "train_generator = tf.data.Dataset.from_generator(\n",
    "    lambda: VideoDataGenerator(train_data),\n",
    "    output_signature=output_signature\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eafa2c2d",
   "metadata": {},
   "source": [
    "val_generator = tf.data.Dataset.from_generator(\n",
    "    lambda: VideoDataGenerator(val_data),\n",
    "    output_signature=output_signature\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0d39bc85",
   "metadata": {},
   "source": [
    "test_generator = tf.data.Dataset.from_generator(\n",
    "    lambda: VideoDataGenerator(test_data),\n",
    "    output_signature=output_signature\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6e18b574",
   "metadata": {},
   "source": [
    "# Inputs\n",
    "facial_input = Input(shape=(30, 224, 224, 3), name='frames_input')\n",
    "micro_exp_inputs = Input(shape=(30, 64, 64, 3), name='micro_exp_input')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9cc1023c",
   "metadata": {},
   "source": [
    "# Define the final model\n",
    "dfds_model = Model(inputs=[facial_input, micro_exp_inputs], outputs=final_model.output)\n",
    "\n",
    "# Compile the model\n",
    "dfds_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0b42713e",
   "metadata": {},
   "source": [
    "dfds_model.input_shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "182aba12",
   "metadata": {},
   "source": [
    "train_generator"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d0894e0d",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8a7d22d",
   "metadata": {},
   "source": [
    "import pickle\n",
    "from datamaker import VideoDataGenerator\n",
    "import tensorflow as tf\n",
    "from deliver import deliver_model\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "with open('../data/video_data_2.pkl', 'rb') as f:\n",
    "    pickled_data = pickle.load(f)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert your video_data dictionary to a list of items for easier splitting\n",
    "data_items = list(pickled_data.items())\n",
    "video_names, labels = zip(*[(video_name, video_info['frame_label'][0]) for video_name, video_info in pickled_data.items()])\n",
    "\n",
    "# Split the data\n",
    "train_names, temp_names, train_labels, temp_labels = train_test_split(video_names, labels, test_size=0.3, random_state=42)\n",
    "val_names, test_names, val_labels, test_labels = train_test_split(temp_names, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Prepare dictionaries for each split\n",
    "train_data = {name: pickled_data[name] for name in train_names}\n",
    "val_data = {name: pickled_data[name] for name in val_names}\n",
    "test_data = {name: pickled_data[name] for name in test_names}\n",
    "\n",
    "# Define the output signature for the generator\n",
    "output_signature = (\n",
    "    (\n",
    "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32)\n",
    "    ),\n",
    "    tf.TensorSpec(shape=(None,1), dtype=tf.float32)\n",
    ")\n",
    "\n",
    "train_generator = tf.data.Dataset.from_generator(\n",
    "    lambda: VideoDataGenerator(train_data),\n",
    "    output_signature=output_signature\n",
    ")\n",
    "\n",
    "val_generator = tf.data.Dataset.from_generator(\n",
    "    lambda: VideoDataGenerator(val_data),\n",
    "    output_signature=output_signature\n",
    ")\n",
    "\n",
    "test_generator = tf.data.Dataset.from_generator(\n",
    "    lambda: VideoDataGenerator(test_data),\n",
    "    output_signature=output_signature\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe75c4f8",
   "metadata": {},
   "source": [
    "from deliver import deliver_model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d3d76540",
   "metadata": {},
   "source": [
    "model_test_1 = deliver_model()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5d62ede2",
   "metadata": {},
   "source": [
    "model_test_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e33c930",
   "metadata": {},
   "source": [
    "model_test_1.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "066b0ec2",
   "metadata": {},
   "source": [
    "model_test_1.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_data=val_generator\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c49646d",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
