{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d427c0897c34fdf8",
   "metadata": {},
   "source": [
    "# Introduction to the Integrated Deepfake Detection System\n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to the Integrated Deepfake Detection System! This project is a comprehensive effort to tackle the growing problem of deepfake videos using advanced machine learning techniques. As deepfake technology continues to evolve, it poses significant challenges to privacy, security, and authenticity in media. Our system is designed to detect deepfakes by analyzing multiple aspects of video content, specifically focusing on spatial, temporal, and micro-expression features. By integrating these different feature types, we aim to create a robust and reliable detection mechanism capable of identifying manipulated video content.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The primary objective of this project is to develop a deepfake detection system that can accurately distinguish between genuine and manipulated videos. This involves:\n",
    "\n",
    "1. **Spatial Feature Extraction**: Analyzing individual frames of a video to capture static facial features. This is accomplished using pre-trained Convolutional Neural Networks (CNNs) such as ResNet50 and VGG16, which are fine-tuned for this task.\n",
    "   \n",
    "2. **Temporal Feature Extraction**: Understanding how facial features change over time by analyzing sequences of frames. Bidirectional Long Short-Term Memory (BiLSTM) networks are employed to capture temporal dependencies and detect inconsistencies in the flow of facial expressions.\n",
    "\n",
    "3. **Micro-Expression Analysis**: Focusing on subtle facial movements that are difficult to replicate in deepfake videos. This module uses specialized CNN architectures to extract and analyze micro-expressions, providing an additional layer of detection.\n",
    "\n",
    "4. **Feature Fusion**: Combining spatial, temporal, and micro-expression features using attention mechanisms to form a comprehensive feature set that enhances detection accuracy. This fusion approach leverages the strengths of each feature type to make a final decision about the authenticity of the video.\n",
    "\n",
    "## Why This Approach?\n",
    "\n",
    "Deepfake detection is a challenging task due to the sophistication of the algorithms used to create these fakes. Traditional detection methods that rely on a single type of feature often fail to capture the complexity of manipulations. By integrating spatial, temporal, and micro-expression features, our approach provides a multi-dimensional analysis that significantly improves the likelihood of detecting deepfakes. This holistic strategy addresses the limitations of existing methods and provides a more reliable solution.\n",
    "\n",
    "### Key Challenges Addressed:\n",
    "\n",
    "- **Variability in Deepfake Techniques**: Different deepfake algorithms have varying strengths and weaknesses. By analyzing multiple aspects of the video, our system can detect a wide range of manipulations.\n",
    "- **Subtle Manipulations**: Some deepfakes are so well-crafted that the manipulations are not immediately noticeable to the human eye. Micro-expression analysis helps in detecting these subtle manipulations.\n",
    "- **Generalization**: Ensuring that the model is not overfitting to specific datasets or types of deepfakes. The fusion of different feature types helps the model generalize better across diverse datasets.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "For this project, we use the **FaceForensics++** dataset, which is a benchmark dataset commonly used in deepfake detection research. It contains a collection of both original and manipulated video sequences, providing a diverse set of examples for training and testing the system. The dataset is divided into two main categories:\n",
    "\n",
    "- **Original Sequences**: Videos that have not been altered, serving as ground truth for authenticity.\n",
    "- **Manipulated Sequences**: Videos that have been altered using various deepfake techniques, providing examples of fake content.\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "This Jupyter Notebook is structured to guide you through the different stages of the deepfake detection process:\n",
    "\n",
    "1. **Data Preprocessing**: Preparing the video frames for feature extraction, including face detection, alignment, and normalization.\n",
    "   \n",
    "2. **Model Architecture**: Detailed implementation of the spatial, temporal, and micro-expression feature extraction models. This section includes building and training the CNN (ResNet50, VGG16), BiLSTM, and micro-expression analysis models.\n",
    "\n",
    "3. **Feature Fusion and Classification**: Combining the extracted features and using attention mechanisms to improve the detection accuracy. The final output layer provides the classification result, indicating whether the video is genuine or a deepfake.\n",
    "\n",
    "4. **Evaluation and Results**: Testing the trained model on a set of test videos and evaluating its performance using metrics such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "5. **Conclusion and Future Work**: Summarizing the findings and discussing potential improvements and future directions for research in deepfake detection.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "To run this notebook, ensure that you have all the necessary dependencies installed. The required libraries are listed in the `requirements.txt` file. Use the following command to install them:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Additionally, make sure you have the FaceForensics++ dataset downloaded and placed in the appropriate directory as specified in the notebook.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The Integrated Deepfake Detection System represents a significant step towards addressing the challenges posed by deepfake technology. By leveraging multiple feature extraction methods and incorporating attention mechanisms, this system aims to provide a robust solution for identifying manipulated video content. We hope that this project will contribute to the ongoing efforts to maintain the integrity and authenticity of digital media.\n",
    "\n",
    "Let's get started and dive into the world of deepfake detection!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9540ebef66cb2a0",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3172c61182ab4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1b4580d4832d031",
   "metadata": {},
   "source": [
    "# Preprocessing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da46b4e2841c322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7953a0c42f06ed",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba19f2fcb81df0d0",
   "metadata": {},
   "source": [
    "## 1. Spatial Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T04:02:07.424626Z",
     "start_time": "2024-09-01T04:02:07.418056Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.api.applications.resnet50 import  ResNet50, preprocess_input as resnet_preprocess\n",
    "from keras.api.applications.vgg16 import VGG16, preprocess_input as vgg_preprocess\n",
    "from keras.api.models import  Model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fe90f1b1c190f6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T04:07:49.067275Z",
     "start_time": "2024-09-01T04:07:34.934619Z"
    }
   },
   "outputs": [],
   "source": [
    "base_model = ResNet50(include_top=False, weights='imagenet', pooling='avg', input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd898bcb7d806b01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T04:09:55.741775Z",
     "start_time": "2024-09-01T04:09:55.727072Z"
    }
   },
   "outputs": [],
   "source": [
    "model =  Model(inputs=base_model.input, outputs=base_model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a33ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_preprocess(image_path, target_size=(224,224)):\n",
    "\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.resize(img, target_size)\n",
    "\n",
    "    img = img/255.0\n",
    "    img = (img-0.5)*2.0\n",
    "\n",
    "    img_arr = tf.expand_dims(img, axis=0)\n",
    "\n",
    "    return img_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "105c4db77fb7b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessable_files = []\n",
    "for filename in os.listdir('../frames'):\n",
    "    filepath = os.path.join('../frames',filename)\n",
    "    preprocessable_files.append(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e53bbe32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "preprocessed_images = []\n",
    "for item in preprocessable_files:\n",
    "    preprocessed_img = resnet_preprocess(item)\n",
    "    preprocessed_images.append(preprocessed_img)\n",
    "preprocessed_images_batch = tf.concat(preprocessed_images, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd56e964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_images_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40d98932",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_images = np.array(preprocessed_images_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a848e377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step\n"
     ]
    }
   ],
   "source": [
    "features = model.predict(preprocessed_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "143ce7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 2048)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c81d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
