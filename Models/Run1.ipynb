{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d427c0897c34fdf8",
   "metadata": {},
   "source": [
    "# Introduction to the Integrated Deepfake Detection System\n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to the Integrated Deepfake Detection System! This project is a comprehensive effort to tackle the growing problem of deepfake videos using advanced machine learning techniques. As deepfake technology continues to evolve, it poses significant challenges to privacy, security, and authenticity in media. Our system is designed to detect deepfakes by analyzing multiple aspects of video content, specifically focusing on spatial, temporal, and micro-expression features. By integrating these different feature types, we aim to create a robust and reliable detection mechanism capable of identifying manipulated video content.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The primary objective of this project is to develop a deepfake detection system that can accurately distinguish between genuine and manipulated videos. This involves:\n",
    "\n",
    "1. **Spatial Feature Extraction**: Analyzing individual frames of a video to capture static facial features. This is accomplished using pre-trained Convolutional Neural Networks (CNNs) such as ResNet50 and VGG16, which are fine-tuned for this task.\n",
    "   \n",
    "2. **Temporal Feature Extraction**: Understanding how facial features change over time by analyzing sequences of frames. Bidirectional Long Short-Term Memory (BiLSTM) networks are employed to capture temporal dependencies and detect inconsistencies in the flow of facial expressions.\n",
    "\n",
    "3. **Micro-Expression Analysis**: Focusing on subtle facial movements that are difficult to replicate in deepfake videos. This module uses specialized CNN architectures to extract and analyze micro-expressions, providing an additional layer of detection.\n",
    "\n",
    "4. **Feature Fusion**: Combining spatial, temporal, and micro-expression features using attention mechanisms to form a comprehensive feature set that enhances detection accuracy. This fusion approach leverages the strengths of each feature type to make a final decision about the authenticity of the video.\n",
    "\n",
    "## Why This Approach?\n",
    "\n",
    "Deepfake detection is a challenging task due to the sophistication of the algorithms used to create these fakes. Traditional detection methods that rely on a single type of feature often fail to capture the complexity of manipulations. By integrating spatial, temporal, and micro-expression features, our approach provides a multi-dimensional analysis that significantly improves the likelihood of detecting deepfakes. This holistic strategy addresses the limitations of existing methods and provides a more reliable solution.\n",
    "\n",
    "### Key Challenges Addressed:\n",
    "\n",
    "- **Variability in Deepfake Techniques**: Different deepfake algorithms have varying strengths and weaknesses. By analyzing multiple aspects of the video, our system can detect a wide range of manipulations.\n",
    "- **Subtle Manipulations**: Some deepfakes are so well-crafted that the manipulations are not immediately noticeable to the human eye. Micro-expression analysis helps in detecting these subtle manipulations.\n",
    "- **Generalization**: Ensuring that the model is not overfitting to specific datasets or types of deepfakes. The fusion of different feature types helps the model generalize better across diverse datasets.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "For this project, we use the **FaceForensics++** dataset, which is a benchmark dataset commonly used in deepfake detection research. It contains a collection of both original and manipulated video sequences, providing a diverse set of examples for training and testing the system. The dataset is divided into two main categories:\n",
    "\n",
    "- **Original Sequences**: Videos that have not been altered, serving as ground truth for authenticity.\n",
    "- **Manipulated Sequences**: Videos that have been altered using various deepfake techniques, providing examples of fake content.\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "This Jupyter Notebook is structured to guide you through the different stages of the deepfake detection process:\n",
    "\n",
    "1. **Data Preprocessing**: Preparing the video frames for feature extraction, including face detection, alignment, and normalization.\n",
    "   \n",
    "2. **Model Architecture**: Detailed implementation of the spatial, temporal, and micro-expression feature extraction models. This section includes building and training the CNN (ResNet50, VGG16), BiLSTM, and micro-expression analysis models.\n",
    "\n",
    "3. **Feature Fusion and Classification**: Combining the extracted features and using attention mechanisms to improve the detection accuracy. The final output layer provides the classification result, indicating whether the video is genuine or a deepfake.\n",
    "\n",
    "4. **Evaluation and Results**: Testing the trained model on a set of test videos and evaluating its performance using metrics such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "5. **Conclusion and Future Work**: Summarizing the findings and discussing potential improvements and future directions for research in deepfake detection.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "To run this notebook, ensure that you have all the necessary dependencies installed. The required libraries are listed in the `requirements.txt` file. Use the following command to install them:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Additionally, make sure you have the FaceForensics++ dataset downloaded and placed in the appropriate directory as specified in the notebook.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The Integrated Deepfake Detection System represents a significant step towards addressing the challenges posed by deepfake technology. By leveraging multiple feature extraction methods and incorporating attention mechanisms, this system aims to provide a robust solution for identifying manipulated video content. We hope that this project will contribute to the ongoing efforts to maintain the integrity and authenticity of digital media.\n",
    "\n",
    "Let's get started and dive into the world of deepfake detection!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9540ebef66cb2a0",
   "metadata": {},
   "source": [
    "## **Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c3172c61182ab4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:17.354406Z",
     "start_time": "2024-09-11T04:56:16.535360Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d48c4ab535e1de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:17.358775Z",
     "start_time": "2024-09-11T04:56:17.354406Z"
    }
   },
   "outputs": [],
   "source": [
    "original_video_directory = '../Datasets/original_sequences'\n",
    "manipulated_video_directory = '../Datasets/manipulated_sequences'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96170d76db4d8ca9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:17.368732Z",
     "start_time": "2024-09-11T04:56:17.358775Z"
    }
   },
   "outputs": [],
   "source": [
    "video_paths = []\n",
    "labels = []\n",
    "\n",
    "for  video in os.listdir(original_video_directory):\n",
    "    if video.endswith('.mp4'):\n",
    "        video_paths.append(os.path.join(original_video_directory, video))\n",
    "        labels.append(0)\n",
    "for video in os.listdir(manipulated_video_directory):\n",
    "    if video.endswith('.mp4'):\n",
    "        video_paths.append(os.path.join(manipulated_video_directory,video))\n",
    "        labels.append(1)\n",
    "\n",
    "deepfake_data = pd.DataFrame({\n",
    "    'video_path': video_paths,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "deepfake_data = deepfake_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4967b47ec0037174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:17.377460Z",
     "start_time": "2024-09-11T04:56:17.368732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    56\n",
       "0    36\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepfake_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a69a5881439a664b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:17.381186Z",
     "start_time": "2024-09-11T04:56:17.377460Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../Datasets/manipulated_sequences\\02_01__outsi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../Datasets/manipulated_sequences\\01_15__exit_...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../Datasets/original_sequences\\12__podium_spee...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../Datasets/original_sequences\\05__kitchen_sti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../Datasets/original_sequences\\08__outside_tal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          video_path  label\n",
       "0  ../Datasets/manipulated_sequences\\02_01__outsi...      1\n",
       "1  ../Datasets/manipulated_sequences\\01_15__exit_...      1\n",
       "2  ../Datasets/original_sequences\\12__podium_spee...      0\n",
       "3  ../Datasets/original_sequences\\05__kitchen_sti...      0\n",
       "4  ../Datasets/original_sequences\\08__outside_tal...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepfake_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b4580d4832d031",
   "metadata": {},
   "source": [
    "## **Preprocessing Phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5da46b4e2841c322",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:17.384711Z",
     "start_time": "2024-09-11T04:56:17.381186Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7953a0c42f06ed",
   "metadata": {},
   "source": [
    "## **Model Building**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5196a483ec9cd14",
   "metadata": {},
   "source": [
    "## 1. **Facial Feature Extractor Module**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba19f2fcb81df0d0",
   "metadata": {},
   "source": [
    "### 1. Spatial Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:22.987890Z",
     "start_time": "2024-09-11T04:56:17.384711Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.api.layers import TimeDistributed,Input, Flatten\n",
    "from keras.api.applications.resnet50 import  ResNet50, preprocess_input as resnet_preprocess\n",
    "from keras.api.models import  Model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fe90f1b1c190f6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:24.211639Z",
     "start_time": "2024-09-11T04:56:22.987890Z"
    }
   },
   "outputs": [],
   "source": [
    "base_model = ResNet50(include_top=False, weights='imagenet', pooling='avg', input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd898bcb7d806b01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:24.221038Z",
     "start_time": "2024-09-11T04:56:24.211639Z"
    }
   },
   "outputs": [],
   "source": [
    "spatial_model =  Model(inputs=base_model.input, outputs=base_model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a33ea5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:24.226386Z",
     "start_time": "2024-09-11T04:56:24.221038Z"
    }
   },
   "outputs": [],
   "source": [
    "def resnet_preprocess(image_path, target_size=(224,224)):\n",
    "\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.resize(img, target_size)\n",
    "\n",
    "    img = img/255.0\n",
    "    img = (img-0.5)*2.0\n",
    "\n",
    "    img_arr = tf.expand_dims(img, axis=0)\n",
    "\n",
    "    return img_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "105c4db77fb7b21d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:24.234904Z",
     "start_time": "2024-09-11T04:56:24.226386Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessable_files = []\n",
    "for filename in os.listdir('../frames'):\n",
    "    filepath = os.path.join('../frames',filename)\n",
    "    preprocessable_files.append(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e53bbe32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:24.752945Z",
     "start_time": "2024-09-11T04:56:24.234904Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_images = []\n",
    "for item in preprocessable_files:\n",
    "    preprocessed_img = resnet_preprocess(item)\n",
    "    preprocessed_images.append(preprocessed_img)\n",
    "preprocessed_images_batch = tf.concat(preprocessed_images, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd56e964",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:24.758315Z",
     "start_time": "2024-09-11T04:56:24.752945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_images_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40d98932",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:24.783780Z",
     "start_time": "2024-09-11T04:56:24.758315Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_images = np.array(preprocessed_images_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a848e377",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:32.727739Z",
     "start_time": "2024-09-11T04:56:24.783780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step\n"
     ]
    }
   ],
   "source": [
    "features = spatial_model.predict(preprocessed_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2e7cacb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:32.733311Z",
     "start_time": "2024-09-11T04:56:32.727739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 224, 224, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a16e650e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:32.737766Z",
     "start_time": "2024-09-11T04:56:32.733311Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_images_expanded = np.expand_dims(preprocessed_images, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8586f98b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:32.743810Z",
     "start_time": "2024-09-11T04:56:32.737766Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 146, 224, 224, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_images_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "143ce7c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:32.748863Z",
     "start_time": "2024-09-11T04:56:32.743810Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 2048)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71ade45a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:32.754486Z",
     "start_time": "2024-09-11T04:56:32.748863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.00772802 ... 4.02363    0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f30f05",
   "metadata": {},
   "source": [
    "### 2. Temporal Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2294abec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:32.763612Z",
     "start_time": "2024-09-11T04:56:32.754486Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Bidirectional,LSTM,Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "601c81d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:32.769920Z",
     "start_time": "2024-09-11T04:56:32.763612Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_temporal_feature_extractor(sequence_length, feature_shape):\n",
    "    input_seq = Input((sequence_length, feature_shape))\n",
    "\n",
    "    lstm_1 = LSTM(128, return_sequences=True, dropout=0.2)\n",
    "    lstm_2 = LSTM(64,return_sequences=True, dropout=0.2)\n",
    "\n",
    "    lstm_out = Bidirectional(lstm_1)(input_seq)\n",
    "    lstm_out = Bidirectional(lstm_2)(lstm_out)\n",
    "\n",
    "    model = Model(inputs=input_seq, outputs=lstm_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9a1b710",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:32.861415Z",
     "start_time": "2024-09-11T04:56:32.769920Z"
    }
   },
   "outputs": [],
   "source": [
    "temporal_feature_extractor = build_temporal_feature_extractor(sequence_length=features.shape[0], feature_shape=features.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10022d92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:32.879681Z",
     "start_time": "2024-09-11T04:56:32.861415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">146</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">146</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,229,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">146</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m146\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m146\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m2,229,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m146\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m164,352\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,393,600</span> (9.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,393,600\u001b[0m (9.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,393,600</span> (9.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,393,600\u001b[0m (9.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temporal_feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c84b20a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:32.884464Z",
     "start_time": "2024-09-11T04:56:32.879681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 2048)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e9938aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:32.889188Z",
     "start_time": "2024-09-11T04:56:32.884464Z"
    }
   },
   "outputs": [],
   "source": [
    "features_temp = np.expand_dims(features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7da060aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:32.895729Z",
     "start_time": "2024-09-11T04:56:32.889188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 146, 2048)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60360263",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:33.479963Z",
     "start_time": "2024-09-11T04:56:32.895729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 561ms/step\n"
     ]
    }
   ],
   "source": [
    "temp_features_extracted = temporal_feature_extractor.predict(features_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfaf86d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:33.485373Z",
     "start_time": "2024-09-11T04:56:33.479963Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.06738199,  0.08338698, -0.00320563, ...,  0.03455913,\n",
       "         -0.08465538, -0.48169863],\n",
       "        [-0.12192948,  0.17146903,  0.00371655, ...,  0.02755469,\n",
       "         -0.09819572, -0.47330675],\n",
       "        [-0.16106763,  0.24418211,  0.01061229, ...,  0.02497226,\n",
       "         -0.10267814, -0.46580225],\n",
       "        ...,\n",
       "        [-0.26383302,  0.41573468,  0.0013749 , ...,  0.0721532 ,\n",
       "         -0.0936614 , -0.04470664],\n",
       "        [-0.2691552 ,  0.4132893 ,  0.0051184 , ...,  0.05087633,\n",
       "         -0.08179858, -0.01124781],\n",
       "        [-0.27371392,  0.3983916 ,  0.01492638, ...,  0.02353151,\n",
       "         -0.05558356,  0.00660072]]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_features_extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8839ed419f1aec",
   "metadata": {},
   "source": [
    "## **2. Micro Expression Inconsistency Detection Module**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7483995b04b067",
   "metadata": {},
   "source": [
    "### **1. Micro Expression Feature Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e395e308720792",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:33.491710Z",
     "start_time": "2024-09-11T04:56:33.485373Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.api.layers import Conv2D,BatchNormalization,Activation,MaxPooling2D,Dropout, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bffd388828a69382",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:33.497961Z",
     "start_time": "2024-09-11T04:56:33.492719Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_micro_exp_spatial_feature_extractor(spatial_input_shape):\n",
    "    spatial_inputs = Input(spatial_input_shape)\n",
    "\n",
    "    # Layer 1\n",
    "    micro_exp_x = Conv2D(32,(3,3),padding='same')(spatial_inputs)\n",
    "    micro_exp_x = BatchNormalization()(micro_exp_x)\n",
    "    micro_exp_x = Activation('relu')(micro_exp_x)\n",
    "    micro_exp_x = MaxPooling2D(pool_size=(2,2))(micro_exp_x)\n",
    "    \n",
    "    # Layer 2\n",
    "    micro_exp_x = Conv2D(64,(3,3),padding='same')(micro_exp_x)\n",
    "    micro_exp_x = BatchNormalization()(micro_exp_x)\n",
    "    micro_exp_x = Activation('relu')(micro_exp_x)\n",
    "    micro_exp_x = MaxPooling2D(pool_size=(2,2))(micro_exp_x)\n",
    "    \n",
    "    # Layer 3\n",
    "    micro_exp_x = Conv2D(128,(3,3),padding='same')(micro_exp_x)\n",
    "    micro_exp_x = BatchNormalization()(micro_exp_x)\n",
    "    micro_exp_x = Activation('relu')(micro_exp_x)\n",
    "    micro_exp_x = MaxPooling2D(pool_size=(2,2))(micro_exp_x)\n",
    "    \n",
    "    # Layer 4\n",
    "    micro_exp_x = Flatten()(micro_exp_x)\n",
    "    micro_exp_x = Dense(256, activation='relu')(micro_exp_x)\n",
    "    # Add dropouts in case of overfitting\n",
    "    \n",
    "    micro_exp_output = Dense(128, activation='relu')(micro_exp_x)\n",
    "    \n",
    "    micro_exp_spatial_feature_extractor = Model(inputs=spatial_inputs, outputs=micro_exp_output)\n",
    "    \n",
    "    return micro_exp_spatial_feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6074e1bea923d5f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:33.502919Z",
     "start_time": "2024-09-11T04:56:33.497961Z"
    }
   },
   "outputs": [],
   "source": [
    "# input_shape = (64,64,3)\n",
    "# inputs = Input(input_shape)\n",
    "# \n",
    "# # Layer 1\n",
    "# x = Conv2D(32,(3,3),padding='same')(inputs)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation('relu')(x)\n",
    "# x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "# \n",
    "# # Layer 2\n",
    "# x = Conv2D(64,(3,3),padding='same')(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation('relu')(x)\n",
    "# x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "# \n",
    "# # Layer 3\n",
    "# x = Conv2D(128,(3,3),padding='same')(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation('relu')(x)\n",
    "# x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "# \n",
    "# # Layer 4\n",
    "# x = Flatten()(x)\n",
    "# x = Dense(256, activation='relu')(x)\n",
    "# # Add dropouts in case of overfitting\n",
    "# \n",
    "# output = Dense(128, activation='relu')(x)\n",
    "# \n",
    "# micro_exp_feature_extractor = Model(inputs=inputs, outputs=output)\n",
    "# micro_exp_feature_extractor.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "# micro_exp_feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc7e9ddfed94684",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:33.507085Z",
     "start_time": "2024-09-11T04:56:33.502919Z"
    }
   },
   "outputs": [],
   "source": [
    "# #TODO: Train the model\n",
    "# micro_exp_feature_extractor.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc25588dbec308d",
   "metadata": {},
   "source": [
    "### **2. Temporal Inconsistency Detection in Micro Expression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e8d6e0b07c3d7cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:33.512393Z",
     "start_time": "2024-09-11T04:56:33.507085Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.api.layers import Attention,Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5e07911f8822c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:33.518244Z",
     "start_time": "2024-09-11T04:56:33.512393Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_temporal_inconsistency_attention(seq_length, feature_dims):\n",
    "    temp_feat_shape = (seq_length, feature_dims)\n",
    "    temp_inputs = Input(shape=temp_feat_shape)\n",
    "    \n",
    "    x_mic_exp = Bidirectional(LSTM(128, return_sequences=True, dropout=0.2))(temp_inputs)\n",
    "    \n",
    "    x_mic_exp = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2))(x_mic_exp)\n",
    "    \n",
    "    attention_output = Attention()([x_mic_exp,x_mic_exp])\n",
    "    \n",
    "    x_mic_exp = Dense(256, activation='relu')(attention_output)\n",
    "    # Add dropout layers for overfitting\n",
    "    x_mic_exp = Dense(128, activation='relu')(x_mic_exp)\n",
    "    \n",
    "    mic_exp_temp_model = Model(inputs=temp_inputs, outputs=x_mic_exp)\n",
    "    \n",
    "    mic_exp_temp_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    \n",
    "    return mic_exp_temp_model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d43392f9d13e1a",
   "metadata": {},
   "source": [
    "## **3. Feature Fusion Layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84699328876087a",
   "metadata": {},
   "source": [
    "### **1. Spatial Attention Mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48e2125903b88b25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:33.522996Z",
     "start_time": "2024-09-11T04:56:33.518244Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.api.layers import Multiply, GlobalAvgPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8122e940e8373bf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:33.527975Z",
     "start_time": "2024-09-11T04:56:33.522996Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_spatial_attention_mechanism(feature_maps):\n",
    "    \"\"\"\n",
    "    :param feature_maps: \n",
    "    :return: weighted feature maps\n",
    "    \"\"\"\n",
    "    \n",
    "    attention_map = Conv2D(1,(1,1),strides=(1,1),padding=\"same\")(feature_maps)\n",
    "    \n",
    "    attention_map = Activation('sigmoid')(attention_map) # 'sigmoid' or 'softmax' can be used as an activation function\n",
    "    \n",
    "    # Element wise multiplication of feature_maps and attention_map\n",
    "    weighted_feature_map = Multiply()([feature_maps, attention_map])\n",
    "    \n",
    "    # Convert the weighted feature map into a context vector\n",
    "    spatial_context_vectors = GlobalAvgPool2D()(weighted_feature_map)\n",
    "    \n",
    "    return spatial_context_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92292e08c44b7b7b",
   "metadata": {},
   "source": [
    "### **2. Temporal Attention Mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49a9176a1176c8c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:33.532819Z",
     "start_time": "2024-09-11T04:56:33.527975Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_temporal_attention_mechanism(feature_maps):\n",
    "    \"\"\"\n",
    "    :param feature_maps: \n",
    "    :return weighted_feature_maps: \n",
    "    \"\"\"\n",
    "    \n",
    "    temporal_attention_scores = Dense(1, activation='tanh')(feature_maps)\n",
    "    \n",
    "    temporal_attention_weights = Activation('softmax')(temporal_attention_scores)\n",
    "    \n",
    "    weighted_temporal_features = Multiply()([feature_maps, temporal_attention_weights])\n",
    "    \n",
    "    context_vector = tf.reduce_sum(weighted_temporal_features, axis=1)\n",
    "    \n",
    "    return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad6d4050af9efff",
   "metadata": {},
   "source": [
    "### **3. Micro Expression Attention Mechanism**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6df99f573ee5e2c",
   "metadata": {},
   "source": [
    "#### **1. Spatial Micro Expression Attention Mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f16ab0296575dd91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:33.538094Z",
     "start_time": "2024-09-11T04:56:33.532819Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_spatial_micro_expression_attention_mechanism(micro_exp_spatial_feature_maps):\n",
    "    \"\"\"\n",
    "    :param micro_exp_spatial_feature_maps: \n",
    "    :return weighted micro_exp feature maps : \n",
    "    \"\"\"\n",
    "    \n",
    "    attention_map = Conv2D(1,(1,1),strides=(1,1),padding=\"same\")(micro_exp_spatial_feature_maps)\n",
    "    \n",
    "    attention_map = Activation('sigmoid')(attention_map)\n",
    "    \n",
    "    weighted_micro_exp_feature_map = Multiply()([micro_exp_spatial_feature_maps,attention_map])\n",
    "    \n",
    "    micro_exp_spatial_context_vector = GlobalAvgPool2D()(weighted_micro_exp_feature_map)\n",
    "    \n",
    "    return micro_exp_spatial_context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712249ac1c4655a1",
   "metadata": {},
   "source": [
    "#### **2. Temporal Micro Expression Attention Mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d70c9acf70368e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:33.542880Z",
     "start_time": "2024-09-11T04:56:33.538094Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_temporal_micro_expression_attention_mechanism(micro_exp_feature_vectors):\n",
    "    \"\"\"\n",
    "    :param micro_exp_feature_vectors: \n",
    "    :return micro_exp_context_vectors: \n",
    "    \"\"\"\n",
    "    \n",
    "    attention_scores = Dense(1,activation='tanh')(micro_exp_feature_vectors)\n",
    "    \n",
    "    attention_weights = Activation('softmax')(attention_scores)\n",
    "    \n",
    "    weighted_micro_exp_temporal_features = Multiply()([attention_weights, micro_exp_feature_vectors])\n",
    "    \n",
    "    micro_exp_context_vector = tf.reduce_sum(weighted_micro_exp_temporal_features, axis=1)\n",
    "    \n",
    "    return micro_exp_context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dec02a110a1397",
   "metadata": {},
   "source": [
    "### **4. Concatenation Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "286bd7dea56315f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:33.549084Z",
     "start_time": "2024-09-11T04:56:33.542880Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.api.layers import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23573323f25ea3f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:33.554654Z",
     "start_time": "2024-09-11T04:56:33.549084Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_feature_fusion_layer(spatial_features, temporal_features, micro_exp_spatial_features, micro_exp_temporal_features):\n",
    "    \n",
    "    spatial_context_vectors = build_spatial_attention_mechanism(feature_maps=spatial_features)\n",
    "\n",
    "    temporal_context_vector = build_temporal_attention_mechanism(feature_maps=temporal_features)\n",
    "    \n",
    "    micro_exp_spatial_context_vector = build_spatial_micro_expression_attention_mechanism(micro_exp_spatial_feature_maps=micro_exp_spatial_features)\n",
    "    \n",
    "    micro_exp_temporal_context_vector = build_temporal_micro_expression_attention_mechanism(micro_exp_feature_vectors=micro_exp_temporal_features)\n",
    "    \n",
    "    concatenated_feature_vector = Concatenate()([\n",
    "        spatial_context_vectors,\n",
    "        temporal_context_vector,\n",
    "        micro_exp_spatial_context_vector,\n",
    "        micro_exp_temporal_context_vector\n",
    "    ])\n",
    "    \n",
    "    return concatenated_feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba6f7f72890a65",
   "metadata": {},
   "source": [
    "## **4. Face Swap Detection Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fe449d4d7cabf57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:33.559392Z",
     "start_time": "2024-09-11T04:56:33.554654Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_face_swap_detection_model(concatenated_feature_vector):\n",
    "    \n",
    "    dense_units = [256,128,64]\n",
    "    \n",
    "    x_face_swap = concatenated_feature_vector\n",
    "    for unit in dense_units:\n",
    "        x_face_swap = Dense(unit, activation='relu')(x_face_swap)\n",
    "        x_face_swap = Dropout(0.5)(x_face_swap)\n",
    "    \n",
    "    op_face_swap = Dense(1,activation='sigmoid')(x_face_swap)\n",
    "    \n",
    "    face_swap_detector_model = Model(inputs=concatenated_feature_vector, output=op_face_swap)\n",
    "    \n",
    "    return face_swap_detector_model    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b634cffad470773",
   "metadata": {},
   "source": [
    "## **Face Swap Detection Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3e4545488a1ac3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:33.565064Z",
     "start_time": "2024-09-11T04:56:33.559392Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_face_swap_detection_pipeline(spatial_feature_dims, temporal_feature_dims, micro_exp_spatial_feature_dims, micro_exp_temporal_feature_dims, temporal_seq_length, micro_exp_temp_seq_length):\n",
    "\n",
    "    #define models for feature extractor modules\n",
    "    spatial_feature_input =  Input(shape=(spatial_feature_dims,))\n",
    "    temporal_feature_input = Input(shape=(temporal_feature_dims,))\n",
    "    micro_exp_spatial_feature_input = Input(shape=(micro_exp_spatial_feature_dims,))\n",
    "    micro_exp_temporal_feature_input = Input(shape=(micro_exp_temporal_feature_dims,))\n",
    "    \n",
    "    # build feature extractor modules\n",
    "    spatial_feature_extractor = spatial_model\n",
    "    temporal_feature_extractor_ = build_temporal_feature_extractor(temporal_seq_length,(temporal_feature_dims,))\n",
    "    micro_exp_spatial_feature_extractor = build_micro_exp_spatial_feature_extractor(micro_exp_spatial_feature_input)\n",
    "    micro_exp_temporal_inconsistency_detector = build_temporal_inconsistency_attention(micro_exp_temp_seq_length,micro_exp_temporal_feature_input)\n",
    "    \n",
    "    #extract features\n",
    "    spatial_features = spatial_feature_extractor()\n",
    "    micro_exp_spatial_features = micro_exp_spatial_feature_extractor()\n",
    "    temporal_features = temporal_feature_extractor(spatial_features)\n",
    "    micro_exp_temporal_features = micro_exp_temporal_inconsistency_detector(micro_exp_spatial_features)\n",
    "    \n",
    "    #build the feature fusion layer with the extracted features as an input\n",
    "    concatenated_feature_vector = build_feature_fusion_layer(spatial_features, temporal_features, micro_exp_spatial_features, micro_exp_temporal_features)\n",
    "    \n",
    "    \n",
    "    #now build the final face detection classification model\n",
    "    face_swap_detector_model = build_face_swap_detection_model(concatenated_feature_vector)\n",
    "    \n",
    "    \n",
    "    return face_swap_detector_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b6a3277e39628a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T04:56:33.568506Z",
     "start_time": "2024-09-11T04:56:33.565064Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
